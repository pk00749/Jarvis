# Copyright (c) 2024 Alibaba Inc (authors: Xiang Lyu)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import os
import time
from tqdm import tqdm
from hyperpyyaml import load_hyperpyyaml
from modelscope import snapshot_download
import torch
from cosyvoice.cli.frontend import CosyVoiceFrontEnd
from cosyvoice.cli.model import CosyVoice2Model
from cosyvoice.utils.file_utils import logging
from cosyvoice.utils.class_utils import get_model_type
from typing import Generator


class CosyVoice2:
    def __init__(self, model_dir, load_jit=False, load_trt=False, fp16=False):
        self.instruct = True if '-Instruct' in model_dir else False
        self.model_dir = model_dir
        self.fp16 = fp16
        if not os.path.exists(model_dir):
            model_dir = snapshot_download(model_dir)
        with open(f'{model_dir}/cosyvoice.yaml', 'r') as f:
            configs = load_hyperpyyaml(f, overrides={'qwen_pretrain_path': os.path.join(model_dir, 'CosyVoice-BlankEN')})
        assert get_model_type(configs) == CosyVoice2Model, f'Do not use {model_dir} for CosyVoice2 initialization!'
        self.frontend = CosyVoiceFrontEnd(configs['get_tokenizer'], configs['feat_extractor'],
                                          f'{model_dir}/campplus.onnx',
                                          f'{model_dir}/speech_tokenizer_v2.onnx',
                                          f'{model_dir}/spk2info.pt',
                                          configs['allowed_special'])
        self.sample_rate = configs['sample_rate']
        if torch.cuda.is_available() is False and (load_jit is True or load_trt is True or fp16 is True):
            load_jit, load_trt, fp16 = False, False, False
            logging.warning('no cuda device, set load_jit/load_trt/fp16 to False')
        self.model = CosyVoice2Model(configs['llm'], configs['flow'], configs['hift'], fp16)
        self.model.load(f'{model_dir}/llm.pt',
                        f'{model_dir}/flow.pt',
                        f'{model_dir}/hift.pt')
        if load_jit:
            self.model.load_jit('{}/flow.encoder.{}.zip'.format(model_dir, 'fp16' if self.fp16 is True else 'fp32'))

        del configs

    def inference_instruct2(self, tts_text, instruct_text, prompt_speech_16k, stream=False, speed=1.0, text_frontend=True):
        assert isinstance(self.model, CosyVoice2Model), 'inference_instruct2 is only implemented for CosyVoice2!'

        # æµå¼å¤„ç†ä¼˜åŒ–ï¼šé¢„å¤„ç†å’Œç¼“å­˜
        normalized_texts = list(self.frontend.text_normalize(tts_text, split=True, text_frontend=text_frontend))

        # é¢„çƒ­æ¨¡å‹ä»¥å‡å°‘å†·å¯åŠ¨æ—¶é—´
        if stream and len(normalized_texts) > 0:
            self._warmup_model_if_needed()

        for i, text_chunk in enumerate(normalized_texts):
            model_input = self.frontend.frontend_instruct2(text_chunk, instruct_text, prompt_speech_16k, self.sample_rate)
            start_time = time.time()
            logging.info('synthesis text {}'.format(text_chunk))
            print(f'synthesis text {text_chunk}')

            # æµå¼å¤„ç†ä¼˜åŒ–ï¼šå¹¶è¡Œå¤„ç†å’Œç¼“å†²
            for j, model_output in enumerate(self.model.tts(**model_input, stream=stream, speed=speed)):
                speech_len = model_output['tts_speech'].shape[1] / self.sample_rate

                # ä¼˜åŒ–éŸ³é¢‘å—å¤„ç†
                if stream:
                    # å®ç°æ™ºèƒ½éŸ³é¢‘å—å¤§å°è°ƒæ•´
                    optimized_output = self._optimize_audio_chunk(model_output, i, j)
                    if optimized_output is not None:
                        yield optimized_output
                else:
                    yield model_output

                # æ€§èƒ½ç›‘æ§ï¼ˆä»…åœ¨è°ƒè¯•æ—¶å¯ç”¨ï¼‰
                if logging.getLogger().isEnabledFor(logging.DEBUG):
                    rtf = (time.time() - start_time) / speech_len if speech_len > 0 else 0
                    logging.debug('yield speech len {}, rtf {}'.format(speech_len, rtf))

                start_time = time.time()

    def _warmup_model_if_needed(self):
        """é¢„çƒ­æ¨¡å‹ä»¥å‡å°‘å†·å¯åŠ¨æ—¶é—´"""
        if not hasattr(self, '_model_warmed') or not self._model_warmed:
            print("ğŸ”¥ é¢„çƒ­æ¨¡å‹ä»¥å‡å°‘å†·å¯åŠ¨æ—¶é—´...")

            try:
                # é¿å…æ— é™é€’å½’ï¼šç›´æ¥ä½¿ç”¨æ¨¡å‹çš„ttsæ–¹æ³•è€Œä¸æ˜¯inference_instruct2
                # åˆ›å»ºæœ€å°çš„æ¨¡å‹è¾“å…¥è¿›è¡Œé¢„çƒ­
                warmup_text = "ä½ å¥½"
                warmup_instruct = "å¥³æ€§"

                # ä½¿ç”¨ç°æœ‰çš„æç¤ºè¯­éŸ³è¿›è¡Œé¢„çƒ­
                from cosyvoice.utils.file_utils import load_wav
                import os

                ROOT_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
                try:
                    warmup_prompt = load_wav(f'{ROOT_DIR}/asset/zero_shot_prompt.wav', 16000)
                except:
                    # å¦‚æœæ— æ³•åŠ è½½æç¤ºè¯­éŸ³ï¼Œåˆ›å»ºä¸€ä¸ªç®€å•çš„é›¶å‘é‡
                    warmup_prompt = torch.zeros(1, 16000)

                # ç›´æ¥è°ƒç”¨frontendå’Œmodelï¼Œé¿å…é€’å½’
                model_input = self.frontend.frontend_instruct2(
                    warmup_text, warmup_instruct, warmup_prompt, self.sample_rate
                )

                # æ‰§è¡Œä¸€æ¬¡å¿«é€Ÿæ¨ç†æ¥é¢„çƒ­æ¨¡å‹ï¼ˆåªå–ç¬¬ä¸€ä¸ªè¾“å‡ºï¼‰
                warmup_output = next(self.model.tts(**model_input, stream=True, speed=1.0), None)

                if warmup_output is not None:
                    self._model_warmed = True
                    print("âœ… æ¨¡å‹é¢„çƒ­å®Œæˆ")
                else:
                    print("âš ï¸ é¢„çƒ­è¾“å‡ºä¸ºç©ºï¼Œè·³è¿‡é¢„çƒ­")
                    self._model_warmed = False

            except Exception as e:
                print(f"âš ï¸ æ¨¡å‹é¢„çƒ­å¤±è´¥ï¼Œå°†è·³è¿‡é¢„çƒ­: {e}")
                self._model_warmed = False

    def _optimize_audio_chunk(self, model_output, chunk_index, audio_index):
        """ä¼˜åŒ–éŸ³é¢‘å—å¤„ç†"""
        if 'tts_speech' not in model_output:
            return None

        audio_chunk = model_output['tts_speech']

        # ä¼˜åŒ–éŸ³é¢‘å—å¤§å° - é’ˆå¯¹Apple Silicon M3ä¼˜åŒ–
        min_chunk_size = 1024  # æœ€å°éŸ³é¢‘å—å¤§å°
        max_chunk_size = 8192  # æœ€å¤§éŸ³é¢‘å—å¤§å°ï¼Œé¿å…å†…å­˜å³°å€¼

        # å¦‚æœéŸ³é¢‘å—å¤ªå°ï¼Œç¼“å†²åå†è¾“å‡º
        if audio_chunk.shape[1] < min_chunk_size and audio_index == 0:
            # å¯¹äºç¬¬ä¸€ä¸ªéŸ³é¢‘å—ï¼Œå¦‚æœå¤ªå°å°±ç¼“å†²
            if not hasattr(self, '_audio_buffer'):
                self._audio_buffer = []
            self._audio_buffer.append(audio_chunk)
            return None

        # å¦‚æœæœ‰ç¼“å†²çš„éŸ³é¢‘ï¼Œåˆå¹¶åè¾“å‡º
        if hasattr(self, '_audio_buffer') and len(self._audio_buffer) > 0:
            buffered_audio = torch.cat(self._audio_buffer + [audio_chunk], dim=1)
            self._audio_buffer = []

            # å¦‚æœåˆå¹¶åçš„éŸ³é¢‘å¤ªå¤§ï¼Œåˆ†å‰²è¾“å‡º
            if buffered_audio.shape[1] > max_chunk_size:
                # è¾“å‡ºå‰åŠéƒ¨åˆ†ï¼Œä¿ç•™ååŠéƒ¨åˆ†
                output_audio = buffered_audio[:, :max_chunk_size]
                self._audio_buffer = [buffered_audio[:, max_chunk_size:]]
                return {'tts_speech': output_audio}
            else:
                return {'tts_speech': buffered_audio}

        # å¦‚æœéŸ³é¢‘å—å¤ªå¤§ï¼Œåˆ†å‰²å¤„ç†
        if audio_chunk.shape[1] > max_chunk_size:
            # è¾“å‡ºå‰åŠéƒ¨åˆ†
            output_audio = audio_chunk[:, :max_chunk_size]
            # ä¿ç•™ååŠéƒ¨åˆ†åˆ°ç¼“å†²åŒº
            if not hasattr(self, '_audio_buffer'):
                self._audio_buffer = []
            self._audio_buffer.append(audio_chunk[:, max_chunk_size:])
            return {'tts_speech': output_audio}

        return model_output

    def inference_zero_shot(self, tts_text, prompt_text, prompt_speech_16k, stream=False, speed=1.0, text_frontend=True):
        prompt_text = self.frontend.text_normalize(prompt_text, split=False, text_frontend=text_frontend)
        for i in tqdm(self.frontend.text_normalize(tts_text, split=True, text_frontend=text_frontend)):
            if (not isinstance(i, Generator)) and len(i) < 0.5 * len(prompt_text):
                logging.warning('synthesis text {} too short than prompt text {}, this may lead to bad performance'.format(i, prompt_text))
            model_input = self.frontend.frontend_zero_shot(i, prompt_text, prompt_speech_16k, self.sample_rate)
            start_time = time.time()
            logging.info('synthesis text {}'.format(i))
            for model_output in self.model.tts(**model_input, stream=stream, speed=speed):
                speech_len = model_output['tts_speech'].shape[1] / self.sample_rate
                logging.info('yield speech len {}, rtf {}'.format(speech_len, (time.time() - start_time) / speech_len))
                yield model_output
                start_time = time.time()