"""Jarvisè¯­éŸ³åŠ©æ‰‹ä¸»ç¨‹åºæ¨¡å—."""

import gc
import os
import sys
from concurrent.futures import ThreadPoolExecutor
from http import HTTPStatus
from typing import Tuple

import gradio as gr
import numpy as np
import pyaudio
from dashscope import Generation
from dashscope.audio.tts_v2 import *
from snownlp import SnowNLP

from cosyvoice.cli.cosyvoice import CosyVoice2
from cosyvoice.utils.file_utils import load_wav
from influence.influence import Influence
from listen.listen import Listen

ROOT_DIR = os.path.dirname(os.path.abspath(__file__))
print(f'Root path: {ROOT_DIR}')
sys.path.append(f'{ROOT_DIR}/third_party/AcademiCodec')
sys.path.append(f'{ROOT_DIR}/third_party/Matcha-TTS')

# é…ç½®é˜¿é‡Œäº‘API
MODEL = "cosyvoice-v2"
VOICE = "longjiayi_v2"

# ç®€åŒ–æ¨¡å¼é…ç½®ï¼šTrueä½¿ç”¨APIæ¨¡å¼ï¼ˆLLM+TTSï¼‰ï¼ŒFalseä½¿ç”¨æœ¬åœ°æ¨¡å¼ï¼ˆLLM+TTSï¼‰
USE_API_MODE = True

# MacBook Air M3 ä¼˜åŒ–: é¢„åŠ è½½æœ¬åœ°æ¨¡å‹åˆ°GPUå†…å­˜ï¼Œä½¿ç”¨fp16å‡å°‘å†…å­˜å ç”¨ï¼ˆä»…åœ¨æœ¬åœ°æ¨¡å¼ä¸‹ï¼‰
cosyvoice = None
if not USE_API_MODE:
    cosyvoice = CosyVoice2(
        f'{ROOT_DIR}/pretrained_models/CosyVoice2-0.5B',
        load_jit=False,
        load_trt=False,
        fp16=True
    )

# MacBook Air M3 ä¼˜åŒ–: åˆ›å»ºçº¿ç¨‹æ± ç”¨äºå¹¶å‘å¤„ç†
executor = ThreadPoolExecutor(max_workers=2)


class APICallbackWithPlayback(ResultCallback):
    """é˜¿é‡Œäº‘APIå›è°ƒç±»ï¼Œæ”¯æŒå®æ—¶éŸ³é¢‘æ’­æ”¾."""

    def __init__(self):
        """åˆå§‹åŒ–å›è°ƒç±»."""
        self._audio_chunks = []
        self._player = None
        self._stream = None

    def on_open(self):
        """WebSocketè¿æ¥æ‰“å¼€æ—¶çš„å›è°ƒ."""
        print("websocket is open.")
        # å¯ç”¨å®æ—¶æ’­æ”¾
        self._player = pyaudio.PyAudio()
        self._stream = self._player.open(
            format=pyaudio.paInt16, channels=1, rate=22050, output=True
        )

    def on_complete(self):
        """è¯­éŸ³åˆæˆå®Œæˆæ—¶çš„å›è°ƒ."""
        print("speech synthesis task success and completed.")

    def on_error(self, message: str):
        """è¯­éŸ³åˆæˆé”™è¯¯æ—¶çš„å›è°ƒ."""
        print(f"speech synthesis task failed, {message}")

    def on_close(self):
        """WebSocketè¿æ¥å…³é—­æ—¶çš„å›è°ƒ."""
        print("websocket is closed.")
        # æ¸…ç†èµ„æº
        if self._stream:
            self._stream.stop_stream()
            self._stream.close()
        if self._player:
            self._player.terminate()

    def on_event(self, message):
        """æ¥æ”¶åˆ°äº‹ä»¶æ¶ˆæ¯æ—¶çš„å›è°ƒ."""
        print(f"Receive speech synthesis message: {message}")

    def on_data(self, data: bytes) -> None:
        """æ¥æ”¶åˆ°éŸ³é¢‘æ•°æ®æ—¶çš„å›è°ƒ."""
        # åŒæ—¶æ”¶é›†å’Œæ’­æ”¾éŸ³é¢‘æ•°æ®
        self._audio_chunks.append(data)
        # å®æ—¶æ’­æ”¾éŸ³é¢‘
        if self._stream:
            self._stream.write(data)

    def get_audio_chunks(self):
        """è·å–éŸ³é¢‘æ•°æ®å—."""
        return self._audio_chunks


def listener(audio):
    """è¯­éŸ³è¯†åˆ«å‡½æ•°."""
    # MacBook Air M3 ä¼˜åŒ–: æ·»åŠ å†…å­˜æ¸…ç†
    gc.collect()
    try:
        if audio is None:
            return "No voice to be recorded."
        filename = Listen.save_voice(audio)
        return Influence.voice_to_text(filename)
    except Exception as e:
        return f"Fail to record voice: {e}"


def _nlp_generator(text):
    """NLPæ–‡æœ¬åˆ†å¥ç”Ÿæˆå™¨."""
    print("Split answer text by NLP...")
    result = SnowNLP(text)
    for sen in result.sentences:
        print(sen)
        yield sen


def brain_streaming_api_with_realtime_playback(audio):
    """ä½¿ç”¨APIæ¨¡å¼çš„è¯­éŸ³å¤„ç†æµç¨‹ï¼Œæ”¯æŒå®æ—¶æ’­æ”¾."""
    gc.collect()

    user_voice_to_text = listener(audio)

    # åˆ›å»ºæ”¯æŒå®æ—¶æ’­æ”¾çš„è¯­éŸ³åˆæˆå™¨
    callback = APICallbackWithPlayback()
    synthesizer = SpeechSynthesizer(model=MODEL, voice=VOICE, format=AudioFormat.PCM_22050HZ_MONO_16BIT, callback=callback,)

    # ä½¿ç”¨é˜¿é‡Œäº‘LLM API
    inf = Influence()
    messages = [{"role": "user", "content": inf._create_cantonese_prompt(user_voice_to_text)}]
    responses = Generation.call(model="qwen-turbo", messages=messages, result_format="message",
                                stream=True, incremental_output=True,
    )

    print("Using API mode with real-time playback (LLM + TTS)...")
    for response in responses:
        if response.status_code == HTTPStatus.OK:
            content = response.output.choices[0]["message"]["content"]
            print(content, end="")
            # æµå¼è°ƒç”¨TTS APIï¼ŒéŸ³é¢‘ä¼šé€šè¿‡callbackå®æ—¶æ’­æ”¾
            synthesizer.streaming_call(content)
        else:
            print(f"Request id:{response.request_id}, Status code:{response.status_code}, "
                  f"error code:{response.code}, error message:{response.message}"
            )

    # å®Œæˆæµå¼åˆæˆ
    synthesizer.streaming_complete()
    print('\nRequestId: ', synthesizer.get_last_request_id())

    # åŒæ—¶ä¹Ÿè¿”å›éŸ³é¢‘æ•°æ®ç»™Gradioï¼ˆå®é™…ä¸Šä¼šè¢«å®æ—¶æ’­æ”¾è¦†ç›–ï¼‰
    audio_chunks = callback.get_audio_chunks()
    for chunk in audio_chunks:
        audio_array = np.frombuffer(chunk, dtype=np.int16).astype(np.float32) / 32768.0
        audio_array = np.clip(audio_array, -1.0, 1.0)
        yield (22050, audio_array)


def brain_streaming_local(audio):
    """ä½¿ç”¨æœ¬åœ°æ¨¡å¼çš„è¯­éŸ³å¤„ç†æµç¨‹ï¼ˆLLMæœ¬åœ° + TTSæœ¬åœ°ï¼‰."""
    # MacBook Air M3 ä¼˜åŒ–: æ™ºèƒ½å†…å­˜ç®¡ç†
    gc.collect()

    user_voice_to_text = listener(audio)
    inf = Influence()

    print("Using local mode (LLM + TTS)...")
    # ä½¿ç”¨æœ¬åœ°LLM
    jarvis_answer_text = inf.llm(user_voice_to_text)
    text_generator = _nlp_generator(jarvis_answer_text)
    print("Using local TTS model for streaming...")
    prompt_speech_16k = load_wav(f'{ROOT_DIR}/asset/zero_shot_prompt.wav', 16000)

    # MacBook Air M3 ä¼˜åŒ–: é¢„æµ‹æ€§èµ„æºé¢„åŠ è½½
    chunk_count = 0

    # ä½¿ç”¨æœ¬åœ°TTSæ¨¡å‹
    for i, j in enumerate(cosyvoice.inference_instruct2(
            tts_text=text_generator,
            instruct_text='ç”¨ç²¤è¯­è¯´è¿™å¥è¯',
            prompt_speech_16k=prompt_speech_16k,
            stream=True)):

        # MacBook Air M3 ä¼˜åŒ–: ä¼˜åŒ–éŸ³é¢‘å¤„ç†æµæ°´çº¿
        audio_chunk = j['tts_speech'].cpu().numpy()
        audio_chunk = np.asarray(audio_chunk, dtype=np.float32)
        audio_chunk = np.nan_to_num(audio_chunk)  # Replace NaN/Inf with 0
        audio_chunk = np.clip(audio_chunk, -1.0, 1.0)

        if audio_chunk.ndim > 1:
            audio_chunk = audio_chunk.flatten()  # Make it 1D

        # MacBook Air M3 ä¼˜åŒ–: ç¼“å†²ç­–ç•¥å‡å°‘å»¶è¿Ÿ
        chunk_count += 1
        if chunk_count % 3 == 0:  # æ¯3ä¸ªchunkæ¸…ç†ä¸€æ¬¡å†…å­˜
            gc.collect()

        yield (24000, audio_chunk)


def brain_streaming(audio):
    """æ ¹æ®é…ç½®é€‰æ‹©ä½¿ç”¨APIæ¨¡å¼è¿˜æ˜¯æœ¬åœ°æ¨¡å¼."""
    if USE_API_MODE:
        return brain_streaming_api_with_realtime_playback(audio)
    else:
        return brain_streaming_local(audio)


def ui_launch():
    """å¯åŠ¨Gradioç•Œé¢."""
    def process_audio_with_mode(audio, mode):
        global USE_API_MODE, cosyvoice
        # æ›´æ–°æ¨¡å¼
        USE_API_MODE = (mode == "APIæ¨¡å¼")

        # å¦‚æœåˆ‡æ¢åˆ°æœ¬åœ°æ¨¡å¼ä¸”æ¨¡å‹æœªåŠ è½½ï¼Œåˆ™åŠ è½½æ¨¡å‹
        if not USE_API_MODE and cosyvoice is None:
            print("Loading local CosyVoice model...")
            cosyvoice = CosyVoice2(f'{ROOT_DIR}/pretrained_models/CosyVoice2-0.5B',
                                 load_jit=False, load_trt=False, fp16=True)

        # å¤„ç†éŸ³é¢‘å¹¶è¿”å›ç»“æœ
        if audio is None:
            return None

        # æ”¶é›†æ‰€æœ‰éŸ³é¢‘chunkså¹¶åˆå¹¶
        audio_chunks = []
        sample_rate = 22050  # é»˜è®¤é‡‡æ ·ç‡

        for chunk in brain_streaming(audio):
            if chunk:
                sample_rate, audio_data = chunk
                audio_chunks.append(audio_data)

        if audio_chunks:
            # åˆå¹¶æ‰€æœ‰éŸ³é¢‘å—
            combined_audio = np.concatenate(audio_chunks)
            return (sample_rate, combined_audio)
        else:
            return None

    # åˆ›å»ºå¸¦æ ‡ç­¾é¡µçš„ç•Œé¢
    with gr.Blocks(title="Jarvis è¯­éŸ³åŠ©æ‰‹ ğŸ‘¾") as demo:
        gr.Markdown("# Jarvis è¯­éŸ³åŠ©æ‰‹ ğŸ‘¾")

        with gr.Tabs():
            # ä¸»åŠŸèƒ½æ ‡ç­¾é¡µ
            with gr.TabItem("è¯­éŸ³åŠ©æ‰‹"):
                gr.Markdown("APIæ¨¡å¼ï¼šLLM API + TTS API | æœ¬åœ°æ¨¡å¼ï¼šLLM æœ¬åœ° + TTS æœ¬åœ°")

                with gr.Row():
                    audio_input = gr.Audio(
                        sources=["microphone"],
                        label="æˆ‘çš„è¯­éŸ³è¾“å…¥",
                        type="numpy"
                    )
                    mode_radio = gr.Radio(
                        choices=["APIæ¨¡å¼", "æœ¬åœ°æ¨¡å¼"],
                        value="APIæ¨¡å¼" if USE_API_MODE else "æœ¬åœ°æ¨¡å¼",
                        label="é€‰æ‹©è¿è¡Œæ¨¡å¼"
                    )

                audio_output = gr.Audio(label="Jarviså›å¤", autoplay=False)

                # å¤„ç†æŒ‰é’®
                process_btn = gr.Button("å¤„ç†è¯­éŸ³", variant="primary")
                process_btn.click(
                    fn=process_audio_with_mode,
                    inputs=[audio_input, mode_radio],
                    outputs=audio_output
                )

            # å”¤é†’è¯è®¾ç½®æ ‡ç­¾é¡µ
            with gr.TabItem("å”¤é†’è¯è®¾ç½®"):
                # ç›´æ¥ä½¿ç”¨è¿ç§»åçš„WakeWordUIComponentsç±»
                wake_word_ui = WakeWordUIComponents()
                wake_word_ui.create_interface()

    demo.launch(inbrowser=True)


def get_wake_word_detector():
    """è·å–å”¤é†’è¯æ£€æµ‹å™¨å®ä¾‹"""
    if not hasattr(get_wake_word_detector, '_instance'):
        try:
            from wake_word import WakeWordDetector, WakeWordConfig
            from wake_word.auto_conversation import AutoConversationHandler

            detector = WakeWordDetector(WakeWordConfig())
            auto_handler = AutoConversationHandler(brain_streaming)

            def handle_wake_detected(result):
                print(f"ğŸ‰ æ£€æµ‹åˆ°å”¤é†’è¯: {result.text} (æƒé‡: {result.weight})")
                auto_handler.handle_wake_up(result)

            detector.set_wake_detected_callback(handle_wake_detected)
            get_wake_word_detector._instance = detector
            print("âœ… å”¤é†’è¯æ£€æµ‹å™¨åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"âŒ å”¤é†’è¯æ£€æµ‹å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            get_wake_word_detector._instance = None

    return getattr(get_wake_word_detector, '_instance', None)


class WakeWordUIComponents:
    """ç²¾ç®€çš„å”¤é†’è¯è®¾ç½®ç•Œé¢ç»„ä»¶"""

    def __init__(self):
        self.detector = get_wake_word_detector()

    def create_interface(self):
        """åˆ›å»ºç²¾ç®€çš„å”¤é†’è¯è®¾ç½®ç•Œé¢"""
        with gr.Blocks(title="å”¤é†’è¯è®¾ç½®") as interface:
            gr.Markdown("# ğŸ¤ å”¤é†’è¯è®¾ç½®")
            gr.Markdown("åŸºäºç²¤è¯­ã€Œå–‚ã€çš„è¯­éŸ³å”¤é†’åŠŸèƒ½")

            with gr.Row():
                # æ§åˆ¶é¢æ¿
                with gr.Column():
                    enabled_checkbox = gr.Checkbox(
                        label="å¯ç”¨å”¤é†’è¯æ£€æµ‹",
                        value=True,
                        info="è¯´ã€Œå–‚ã€æ¥å”¤é†’Jarvis"
                    )

                    sensitivity_slider = gr.Slider(
                        minimum=0.5,
                        maximum=2.0,
                        value=1.2,
                        step=0.1,
                        label="æ•æ„Ÿåº¦",
                        info="æ•°å€¼è¶Šä½è¶Šæ•æ„Ÿ"
                    )

                    with gr.Row():
                        start_btn = gr.Button("å¼€å§‹ç›‘å¬", variant="primary")
                        stop_btn = gr.Button("åœæ­¢ç›‘å¬")

                # çŠ¶æ€æ˜¾ç¤º
                with gr.Column():
                    status_text = gr.Textbox(
                        label="çŠ¶æ€",
                        value="æœªå¯åŠ¨",
                        interactive=False
                    )

                    recognition_text = gr.Textbox(
                        label="è¯†åˆ«ç»“æœ",
                        value="æš‚æ— ",
                        interactive=False
                    )

                    weight_text = gr.Textbox(
                        label="æƒé‡ï¿½ï¿½ï¿½æ¯",
                        value="æƒé‡: 0.0 | è§¦å‘: å¦",
                        interactive=False
                    )

            # äº‹ä»¶ç»‘å®š
            enabled_checkbox.change(
                fn=self._toggle_enabled,
                inputs=[enabled_checkbox],
                outputs=[status_text]
            )

            sensitivity_slider.change(
                fn=self._update_sensitivity,
                inputs=[sensitivity_slider],
                outputs=[status_text]
            )

            start_btn.click(
                fn=self._start_listening,
                outputs=[status_text]
            )

            stop_btn.click(
                fn=self._stop_listening,
                outputs=[status_text]
            )

            # å®šæœŸæ›´æ–°çŠ¶æ€
            interface.load(
                fn=self._update_status,
                outputs=[status_text, recognition_text, weight_text],
                every=3.0
            )

    def _toggle_enabled(self, enabled: bool) -> str:
        """åˆ‡æ¢å¯ç”¨çŠ¶æ€"""
        if not self.detector:
            return "âŒ æ£€æµ‹å™¨æœªåˆå§‹åŒ–"

        if enabled:
            self.detector.enable()
            return "âœ… å·²å¯ç”¨"
        else:
            self.detector.disable()
            return "âŒ å·²ç¦ç”¨"

    def _update_sensitivity(self, sensitivity: float) -> str:
        """æ›´æ–°æ•æ„Ÿåº¦"""
        if self.detector:
            self.detector.update_sensitivity(sensitivity)
        return f"ğŸšï¸ æ•æ„Ÿåº¦: {sensitivity:.1f}"

    def _start_listening(self) -> str:
        """å¼€å§‹ç›‘å¬"""
        if not self.detector:
            return "âŒ æ£€æµ‹å™¨æœªåˆå§‹åŒ–"

        if self.detector.start_detection():
            return "ğŸ¤ æ­£åœ¨ç›‘å¬ä¸­..."
        else:
            return "âŒ å¯åŠ¨å¤±è´¥"

    def _stop_listening(self) -> str:
        """åœæ­¢ç›‘å¬"""
        if self.detector:
            self.detector.stop_detection()
        return "â¹ï¸ å·²åœæ­¢ç›‘å¬"

    def _update_status(self) -> Tuple[str, str, str]:
        """æ›´æ–°çŠ¶æ€æ˜¾ç¤º"""
        if not self.detector:
            return "âŒ æ£€æµ‹å™¨æœªåˆå§‹åŒ–", "æš‚æ— ", "æƒé‡: 0.0"

        status = self.detector.get_status()

        # çŠ¶æ€æ–‡æœ¬
        if status['is_running']:
            status_text = "ğŸ¤ ç›‘å¬ä¸­"
        else:
            status_text = "â­• ç©ºé—²"

        # è¯†åˆ«ç»“æœ
        recognition_text = status.get('current_recognition', '') or "æš‚æ— "

        # æƒé‡ä¿¡æ¯
        history = self.detector.get_history()
        if history:
            latest = history[-1]
            weight_text = f"æƒé‡: {latest['weight']:.1f} | è§¦å‘: {'æ˜¯' if latest['is_triggered'] else 'å¦'}"
        else:
            weight_text = "æƒé‡: 0.0 | è§¦å‘: å¦"

        return status_text, recognition_text, weight_text


if __name__ == "__main__":
    ui_launch()
