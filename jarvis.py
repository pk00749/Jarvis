import gradio as gr
import os, sys
import numpy as np
from influence.influence import Influence
from listen.listen import Listen
from cosyvoice.cli.cosyvoice import CosyVoice2
from cosyvoice.utils.file_utils import load_wav
from snownlp import SnowNLP
import torch
import gc
from concurrent.futures import ThreadPoolExecutor
import time
import scipy.signal  # ä¿¡å·å¤„ç†åº“
from scipy.io import wavfile  # æ·»åŠ WAVæ–‡ä»¶å¤„ç†

ROOT_DIR = os.path.dirname(os.path.abspath(__file__))
print(f'Root path: {ROOT_DIR}')
sys.path.append(f'{ROOT_DIR}/third_party/AcademiCodec')
sys.path.append(f'{ROOT_DIR}/third_party/Matcha-TTS')

# macOS M3æ€§èƒ½ä¼˜åŒ–ï¼šå¯ç”¨ç¡¬ä»¶åŠ é€Ÿå’Œä¼˜åŒ–è®¾ç½®
def initialize_cosyvoice_optimized():
    """
    é’ˆå¯¹MacBook Air M3ä¼˜åŒ–çš„CosyVoice2åˆå§‹åŒ–
    """
    print("ğŸš€ æ­£åœ¨åˆå§‹åŒ–CosyVoice2ï¼Œå¯ç”¨Apple Siliconä¼˜åŒ–...")

    # æ£€æµ‹Apple Siliconå¹¶è®¾ç½®ä¼˜åŒ–å‚æ•°
    is_apple_silicon = hasattr(torch.backends, 'mps') and torch.backends.mps.is_available()

    if is_apple_silicon:
        print("âœ… æ£€æµ‹åˆ°Apple Siliconï¼Œå¯ç”¨MPSåŠ é€Ÿ")
        # è®¾ç½®MPSä¸ºé»˜è®¤è®¾å¤‡
        try:
            torch.backends.mps.empty_cache()
        except:
            pass  # å¿½ç•¥MPSç¼“å­˜æ¸…ç†é”™è¯¯

        # é’ˆå¯¹Apple Siliconä¼˜åŒ–çš„å‚æ•°
        cosyvoice = CosyVoice2(
            f'{ROOT_DIR}/pretrained_models/CosyVoice2-0.5B',
            load_jit=True,      # å¯ç”¨JITç¼–è¯‘ä¼˜åŒ–
            load_trt=False,     # TensorRTåœ¨Apple Siliconä¸Šä¸å¯ç”¨
            fp16=True           # å¯ç”¨åŠç²¾åº¦æµ®ç‚¹ï¼Œå‡å°‘å†…å­˜ä½¿ç”¨
        )
    else:
        print("âš ï¸  æœªæ£€æµ‹åˆ°Apple Siliconï¼Œä½¿ç”¨é»˜è®¤è®¾ç½®")
        cosyvoice = CosyVoice2(
            f'{ROOT_DIR}/pretrained_models/CosyVoice2-0.5B',
            load_jit=False,
            load_trt=False,
            fp16=False
        )

    return cosyvoice

# å…¨å±€å˜é‡å’Œç¼“å­˜ç®¡ç†
cosyvoice = None
model_cache = {}
executor = ThreadPoolExecutor(max_workers=3)  # åˆ©ç”¨å¤šæ ¸å¤„ç†èƒ½åŠ›

def get_cosyvoice():
    """è·å–CosyVoice2å®ä¾‹ï¼Œä½¿ç”¨å•ä¾‹æ¨¡å¼é¿å…é‡å¤åˆå§‹åŒ–"""
    global cosyvoice
    if cosyvoice is None:
        cosyvoice = initialize_cosyvoice_optimized()
    return cosyvoice

def optimize_memory():
    """å†…å­˜ä¼˜åŒ–ï¼šæ¸…ç†ä¸å¿…è¦çš„ç¼“å­˜"""
    global model_cache

    # å¼ºåˆ¶åƒåœ¾å›æ”¶
    gc.collect()

    # æ¸…ç†MPSç¼“å­˜ï¼ˆå¦‚æœå¯ç”¨ï¼‰- ä¿®å¤APIè°ƒç”¨é”™è¯¯
    try:
        if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            # æ£€æŸ¥æ˜¯å¦æœ‰empty_cacheæ–¹æ³•ï¼ˆä¸åŒPyTorchç‰ˆæœ¬å¯èƒ½ä¸åŒï¼‰
            if hasattr(torch.backends.mps, 'empty_cache'):
                torch.backends.mps.empty_cache()
            else:
                # å¤‡é€‰æ–¹æ¡ˆï¼šä½¿ç”¨torch.mps.empty_cache()ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if hasattr(torch, 'mps') and hasattr(torch.mps, 'empty_cache'):
                    torch.mps.empty_cache()
    except Exception as ex:
        # å¿½ç•¥MPSç¼“å­˜æ¸…ç†é”™è¯¯ï¼Œä¸å½±å“ä¸»è¦åŠŸèƒ½
        print(f"âš ï¸ MPSç¼“å­˜æ¸…ç†è·³è¿‡: {ex}")


def listener(audio):
    try:
        if audio is None:
            return "No voice to be recorded."
        filename = Listen.save_voice(audio)
        return Influence.voice_to_text(filename)
    except Exception as e:
        return f"Fail to record voice: {e}"

def _nlp_generator(text):
    print("Split answer text by NLP...")
    result = SnowNLP(text)
    print(result.sentences)
    for sen in result.sentences:
        print(sen)
        yield sen

def brain_streaming_optimized(audio):
    """ä¼˜åŒ–çš„è¯­éŸ³å¤„ç†å‡½æ•° - ä½¿ç”¨æŒ‡ä»¤æ¨¡å¼ç”Ÿæˆç²¤è¯­è¯­éŸ³ - æµå¼è¿”å›éŸ³é¢‘å—"""
    start_time = time.time()

    try:
        # æ­¥éª¤1: è¯­éŸ³è¯†åˆ«
        print("ğŸ¤ å¼€å§‹è¯­éŸ³è¯†åˆ«...")
        prompt_text = listener(audio)
        print(f"âœ… è¯†åˆ«ç»“æœ: {prompt_text}")

        # æ­¥éª¤2: LLMç”Ÿæˆå›å¤
        print("ğŸ§  å¼€å§‹LLMç”Ÿæˆå›å¤...")
        answer_text = Influence.llm(prompt_text)
        print(f"âœ… LLMå›å¤: {answer_text}")

        # æ­¥éª¤3: ä½¿ç”¨æŒ‡ä»¤æ¨¡å¼è¯­éŸ³åˆæˆ - ç”Ÿæˆç²¤è¯­
        print("ğŸµ å¼€å§‹æµå¼ç²¤è¯­è¯­éŸ³åˆæˆ...")
        cosyvoice_instance = get_cosyvoice()

        # ğŸ”§ ç›´æ¥åŠ è½½æç¤ºéŸ³é¢‘
        print("ğŸµ åŠ è½½æç¤ºéŸ³é¢‘...")
        prompt_speech_16k = load_wav(f'{ROOT_DIR}/asset/zero_shot_prompt.wav', 16000)
        print("âœ… æç¤ºéŸ³é¢‘åŠ è½½å®Œæˆ")

        # ğŸ”§ å…³é”®ï¼šä½¿ç”¨æŒ‡ä»¤æ–‡æœ¬æ§åˆ¶è¯­éŸ³ç‰¹æ€§
        instruct_text = "è¯·ç”¨ç²¤è¯­è¯´è¯"  # æ˜ç¡®æŒ‡ä»¤ï¼šä½¿ç”¨ç²¤è¯­

        # ğŸ”§ é¢„å…ˆç”Ÿæˆå¥å­åˆ—è¡¨ï¼Œç„¶ååˆ›å»ºç”Ÿæˆå™¨å‡½æ•°
        print("ğŸ“ å¼€å§‹åˆ†å¥å¤„ç†...")
        sentences = list(_nlp_generator_optimized(answer_text))
        print(f"âœ… åˆ†å¥å®Œæˆï¼Œå…± {len(sentences)} ä¸ªå¥å­:")
        for i, sentence in enumerate(sentences, 1):
            print(f"   å¥å­{i}: ã€Œ{sentence}ã€")

        if not sentences:
            print("âŒ åˆ†å¥ç»“æœä¸ºç©º")
            return

        def create_text_generator():
            """åˆ›å»ºæ–‡æœ¬ç”Ÿæˆå™¨ - ç¡®ä¿åªç”Ÿæˆä¸€æ¬¡"""
            print("ğŸ¯ ç”Ÿæˆå™¨å¼€å§‹å·¥ä½œ...")
            for i, sentence in enumerate(sentences, 1):
                print(f"ğŸ¯ ç”Ÿæˆå™¨äº§å‡ºå¥å­ {i}: ã€Œ{sentence}ã€")
                yield sentence.strip()
            print("ğŸ¯ ç”Ÿæˆå™¨å·¥ä½œå®Œæˆ")

        # æµå¼éŸ³é¢‘ç”Ÿæˆå’Œæ’­æ”¾
        audio_chunk_count = 0
        min_chunk_duration = 0.5  # æœ€å°éŸ³é¢‘å—æ—¶é•¿ï¼ˆç§’ï¼‰ï¼Œé¿å…æ’­æ”¾è¿‡äºé¢‘ç¹
        accumulated_chunk = []

        print("ğŸµ å¼€å§‹æµå¼éŸ³é¢‘ç”Ÿæˆ...")
        print(f"ğŸ“‹ æŒ‡ä»¤æ–‡æœ¬: {instruct_text}")
        print("=" * 60)

        try:
            for i, output in enumerate(cosyvoice_instance.inference_instruct2(
                    create_text_generator(),
                    instruct_text,
                    prompt_speech_16k,
                    stream=True)):

                audio_chunk_count += 1
                chunk_start_time = time.time()

                print(f"ğŸµ æ­£åœ¨å¤„ç†ç¬¬ {audio_chunk_count} ä¸ªéŸ³é¢‘å—...")

                # å¤„ç†éŸ³é¢‘å—
                if 'tts_speech' not in output:
                    print(f"âš ï¸ éŸ³é¢‘å— {audio_chunk_count} ç¼ºå°‘ tts_speech")
                    continue

                audio_chunk = output['tts_speech'].cpu().numpy()

                # ç¡®ä¿æ•°æ®ç±»å‹ä¸ºfloat32
                if audio_chunk.dtype != np.float32:
                    audio_chunk = audio_chunk.astype(np.float32)

                # ç¡®ä¿éŸ³é¢‘ä¸º1Dæ•°ç»„
                if audio_chunk.ndim > 1:
                    audio_chunk = audio_chunk.flatten()

                # éŸ³é¢‘è´¨é‡ä¼˜åŒ–
                audio_chunk = np.clip(audio_chunk, -0.95, 0.95)

                if len(audio_chunk) > 0:
                    # å°†éŸ³é¢‘å—åŠ å…¥ç´¯ç§¯ç¼“å†²åŒº
                    accumulated_chunk.append(audio_chunk)

                    # æ£€æŸ¥ç´¯ç§¯çš„éŸ³é¢‘æ˜¯å¦è¾¾åˆ°æœ€å°æ’­æ”¾æ—¶é•¿
                    total_samples = sum(len(chunk) for chunk in accumulated_chunk)
                    total_duration = total_samples / 22050

                    chunk_duration = len(audio_chunk) / 22050
                    chunk_time = time.time() - chunk_start_time

                    print(f"âœ… éŸ³é¢‘å— {audio_chunk_count} å·²å¤„ç†")
                    print(f"   - å½“å‰å—é•¿åº¦: {len(audio_chunk)} æ ·æœ¬ ({chunk_duration:.2f}ç§’)")
                    print(f"   - ç´¯ç§¯æ—¶é•¿: {total_duration:.2f}ç§’")
                    print(f"   - å¤„ç†ç”¨æ—¶: {chunk_time:.2f}ç§’")

                    # ğŸ”§ å…³é”®ï¼šå½“ç´¯ç§¯éŸ³é¢‘è¾¾åˆ°æœ€å°æ’­æ”¾æ—¶é•¿æ—¶ï¼Œyieldè¿”å›éŸ³é¢‘å—
                    if total_duration >= min_chunk_duration or audio_chunk_count % 3 == 0:
                        # åˆå¹¶ç´¯ç§¯çš„éŸ³é¢‘å—
                        if len(accumulated_chunk) == 1:
                            playback_audio = accumulated_chunk[0]
                        else:
                            playback_audio = np.concatenate(accumulated_chunk)

                        # åº”ç”¨åŸºæœ¬çš„éŸ³é¢‘ä¼˜åŒ–
                        playback_audio = np.clip(playback_audio, -0.95, 0.95)
                        playback_audio = trim_silence(playback_audio, threshold=0.005)

                        # è½»é‡çº§éŸ³é¢‘å¢å¼ºï¼ˆé¿å…è¿‡åº¦å¤„ç†ï¼‰
                        max_val = np.max(np.abs(playback_audio))
                        if max_val > 0:
                            playback_audio = playback_audio / max_val * 0.7

                        playback_duration = len(playback_audio) / 22050

                        print(f"ğŸµ è¿”å›éŸ³é¢‘å—ç”¨äºæ’­æ”¾:")
                        print(f"   - æ’­æ”¾æ—¶é•¿: {playback_duration:.2f}ç§’")
                        print(f"   - åŒ…å« {len(accumulated_chunk)} ä¸ªåŸå§‹éŸ³é¢‘å—")

                        # ğŸ”§ å…³é”®ï¼šyieldè¿”å›éŸ³é¢‘å—ä¾›ç•Œé¢æµå¼æ’­æ”¾
                        yield (22050, playback_audio)

                        # æ¸…ç©ºç´¯ç§¯ç¼“å†²åŒºï¼Œä¸ºä¸‹ä¸€è½®åšå‡†å¤‡
                        accumulated_chunk = []

                        # æ·»åŠ çŸ­æš‚åœé¡¿ï¼Œè®©ç•Œé¢æœ‰æ—¶é—´å¤„ç†
                        time.sleep(0.1)

                else:
                    print(f"âš ï¸ éŸ³é¢‘å— {audio_chunk_count} ä¸ºç©ºï¼Œè·³è¿‡")

                # å†…å­˜ç®¡ç†
                if audio_chunk_count % 5 == 0:
                    optimize_memory()

            # å¤„ç†å‰©ä½™çš„ç´¯ç§¯éŸ³é¢‘ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            if accumulated_chunk:
                print(f"ğŸ”— å¤„ç†å‰©ä½™çš„ {len(accumulated_chunk)} ä¸ªéŸ³é¢‘å—...")

                if len(accumulated_chunk) == 1:
                    final_audio = accumulated_chunk[0]
                else:
                    final_audio = np.concatenate(accumulated_chunk)

                # æœ€ç»ˆéŸ³é¢‘ä¼˜åŒ–
                final_audio = np.clip(final_audio, -0.95, 0.95)
                final_audio = trim_silence(final_audio, threshold=0.005)

                # è½»é‡çº§å¢å¼º
                max_val = np.max(np.abs(final_audio))
                if max_val > 0:
                    final_audio = final_audio / max_val * 0.7

                final_duration = len(final_audio) / 22050
                print(f"ğŸµ è¿”å›æœ€ç»ˆéŸ³é¢‘å—: {final_duration:.2f}ç§’")

                yield (22050, final_audio)

            total_time = time.time() - start_time
            print("=" * 60)
            print(f"âœ… æµå¼éŸ³é¢‘ç”Ÿæˆå®Œæˆ:")
            print(f"   - æ€»éŸ³é¢‘å—æ•°: {audio_chunk_count}")
            print(f"   - æ€»å¤„ç†æ—¶é—´: {total_time:.2f}ç§’")
            print(f"   - ä½¿ç”¨æŒ‡ä»¤: {instruct_text}")
            print("=" * 60)

        except Exception as e:
            print(f"âŒ æµå¼éŸ³é¢‘ç”Ÿæˆå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return

    except Exception as e:
        print(f"âŒ æµå¼éŸ³é¢‘å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return

def _nlp_generator_optimized(text):
    """ä¼˜åŒ–çš„NLPæ–‡æœ¬åˆ†å¥å¤„ç† - é’ˆå¯¹ä¸­æ–‡ä¼˜åŒ–"""
    print(f"ğŸ“ å¼€å§‹æ™ºèƒ½åˆ†å¥ï¼ŒåŸæ–‡: {text}")

    # æ™ºèƒ½ç¼“å­˜
    cache_key = hash(text)
    if cache_key in model_cache:
        sentences = model_cache[cache_key]
        print(f"âœ… ä½¿ç”¨ç¼“å­˜çš„åˆ†å¥ç»“æœ: {len(sentences)} ä¸ªå¥å­")
    else:
        # ä½¿ç”¨SnowNLPè¿›è¡Œä¸­æ–‡åˆ†å¥
        result = SnowNLP(text)
        sentences = []

        for sentence in result.sentences:
            sentence = sentence.strip()
            # è¿‡æ»¤æ‰è¿‡çŸ­çš„å¥å­ï¼ˆå°‘äº2ä¸ªå­—ç¬¦ï¼‰
            if len(sentence) >= 2:
                sentences.append(sentence)

        # å¦‚æœåˆ†å¥ç»“æœä¸ºç©ºï¼Œä½¿ï¿½ï¿½ï¿½åŸæ–‡
        if not sentences:
            sentences = [text.strip()]

        # ç¼“å­˜ç»“æœ
        if len(model_cache) > 50:  # é™åˆ¶ç¼“å­˜å¤§å°
            oldest_keys = list(model_cache.keys())[:10]
            for key in oldest_keys:
                del model_cache[key]

        model_cache[cache_key] = sentences
        print(f"âœ… æ–°åˆ†å¥ç»“æœ: {len(sentences)} ä¸ªå¥å­")

    # è¾“å‡ºåˆ†å¥ç»“æœ
    for i, sentence in enumerate(sentences, 1):
        print(f"   å¥å­{i}: {sentence}")
        yield sentence

def trim_silence(audio, threshold=0.01):
    """å»é™¤éŸ³é¢‘å¼€å¤´å’Œç»“å°¾çš„é™éŸ³éƒ¨åˆ†"""
    # æ‰¾åˆ°å¼€å¤´ç¬¬ä¸€ä¸ªéé™éŸ³æ ·æœ¬
    start_idx = 0
    for i, sample in enumerate(audio):
        if abs(sample) > threshold:
            start_idx = i
            break

    # æ‰¾åˆ°ç»“å°¾æœ€åä¸€ä¸ªéé™éŸ³æ ·æœ¬
    end_idx = len(audio) - 1
    for i in range(len(audio) - 1, -1, -1):
        if abs(audio[i]) > threshold:
            end_idx = i + 1
            break

    return audio[start_idx:end_idx] if start_idx < end_idx else audio

def enhance_audio_clarity(audio, sample_rate=22050):
    """
    éŸ³é¢‘æ¸…æ™°åº¦å¢å¼ºå‡½ï¿½ï¿½
    é’ˆå¯¹è¯­éŸ³æ¸…æ™°åº¦è¿›è¡Œå¤šé‡ä¼˜åŒ–
    """
    print("ğŸ”§ å¼€å§‹éŸ³é¢‘æ¸…æ™°åº¦å¢å¼º...")

    try:
        # 1. éŸ³é¢‘å½’ä¸€åŒ– - æå‡æ•´ä½“éŸ³é‡
        max_val = np.max(np.abs(audio))
        if max_val > 0:
            # ä½¿ç”¨è¾ƒé«˜çš„å½’ä¸€åŒ–ç›®æ ‡å€¼ï¼Œæå‡éŸ³é‡
            normalized_audio = audio / max_val * 0.8  # ä»0.95é™åˆ°0.8é¿å…è¿‡åº¦å¢ç›Š
        else:
            normalized_audio = audio

        # 2. åŠ¨æ€èŒƒå›´å‹ç¼© - è®©å°å£°éŸ³æ›´æ¸…æ¥šï¼Œå¤§å£°éŸ³ä¸è¿‡è½½
        def soft_compress(x, threshold=0.3, ratio=4.0):
            """è½¯å‹ç¼©ç®—æ³•"""
            abs_x = np.abs(x)
            sign_x = np.sign(x)

            # å¯¹è¶…è¿‡é˜ˆå€¼çš„éƒ¨åˆ†è¿›è¡Œå‹ç¼©
            compressed = np.where(
                abs_x > threshold,
                threshold + (abs_x - threshold) / ratio,
                abs_x
            )

            return sign_x * compressed

        compressed_audio = soft_compress(normalized_audio)

        # 3. é«˜é¢‘å¢å¼º - æå‡è¯­éŸ³æ¸…æ™°åº¦å…³é”®é¢‘ç‡
        # è®¾è®¡ä¸€ä¸ªç®€å•çš„é«˜é¢‘å¢å¼ºæ»¤æ³¢å™¨
        nyquist = sample_rate / 2

        # å¢å¼º1-4kHzé¢‘æ®µï¼ˆè¯­éŸ³æ¸…æ™°åº¦å…³é”®é¢‘æ®µï¼‰
        b, a = scipy.signal.butter(2, [1000/nyquist, 4000/nyquist], btype='band')
        high_freq = scipy.signal.filtfilt(b, a, compressed_audio)

        # å°†å¢å¼ºçš„é«˜é¢‘æˆåˆ†æ··åˆå›åŸå§‹éŸ³é¢‘
        enhanced_audio = compressed_audio + high_freq * 0.3

        # 4. å»å™ªå¤„ç† - ç§»é™¤ä½é¢‘å™ªéŸ³
        # é«˜é€šæ»¤æ³¢å™¨ï¼Œç§»é™¤50Hzä»¥ä¸‹çš„å™ªéŸ³
        b_hp, a_hp = scipy.signal.butter(2, 50/nyquist, btype='high')
        denoised_audio = scipy.signal.filtfilt(b_hp, a_hp, enhanced_audio)

        # 5. æœ€ç»ˆé™å¹…å’Œå¹³æ»‘
        final_audio = np.clip(denoised_audio, -0.9, 0.9)

        # 6. éŸ³é¢‘å¢ç›Šè°ƒæ•´ - å†æ¬¡æå‡æ•´ä½“éŸ³é‡
        final_max = np.max(np.abs(final_audio))
        if final_max > 0:
            # ç›®æ ‡RMSå€¼è°ƒæ•´ï¼Œè®©éŸ³é¢‘æ›´å“äº®
            current_rms = np.sqrt(np.mean(final_audio**2))
            target_rms = 0.2  # ç›®æ ‡RMSå€¼ï¼Œæå‡éŸ³é‡
            gain = target_rms / current_rms if current_rms > 0 else 1.0
            final_audio = final_audio * min(gain, 3.0)  # æœ€å¤§3å€å¢ç›Š

        # æœ€ç»ˆé™å¹…
        final_audio = np.clip(final_audio, -0.95, 0.95)

        print("âœ… éŸ³é¢‘æ¸…æ™°åº¦å¢å¼ºå®Œæˆ")
        print(f"   - åŸå§‹RMS: {np.sqrt(np.mean(audio**2)):.4f}")
        print(f"   - å¢å¼ºåRMS: {np.sqrt(np.mean(final_audio**2)):.4f}")
        print(f"   - å¢ç›Šå€æ•°: {np.sqrt(np.mean(final_audio**2))/np.sqrt(np.mean(audio**2)):.2f}x")

        return final_audio

    except Exception as e:
        print(f"âŒ éŸ³é¢‘å¢å¼ºå¤±è´¥: {e}")
        # å¦‚æœå¢å¼ºå¤±è´¥ï¼Œè¿”å›åŸå§‹éŸ³é¢‘ä½†è¿›è¡ŒåŸºæœ¬çš„éŸ³é‡æå‡
        return np.clip(audio * 2.0, -0.95, 0.95)

def brain_streaming_with_realtime_playback(audio):
    """çœŸæ­£çš„æµå¼éŸ³é¢‘ç”Ÿæˆå‡½æ•° - é€ä¸ªè¿”å›éŸ³é¢‘å—ä¾›å®æ—¶æ’­æ”¾"""
    start_time = time.time()

    try:
        # æ­¥éª¤1: è¯­éŸ³è¯†åˆ«
        print("ğŸ¤ å¼€å§‹è¯­éŸ³è¯†åˆ«...")
        prompt_text = listener(audio)
        print(f"âœ… è¯†åˆ«ç»“æœ: {prompt_text}")

        # æ­¥éª¤2: LLMç”Ÿæˆå›å¤
        print("ğŸ§  å¼€å§‹LLMç”Ÿæˆå›å¤...")
        answer_text = Influence.llm(prompt_text)
        print(f"âœ… LLMå›å¤: {answer_text}")

        # æ­¥éª¤3: ä½¿ç”¨æŒ‡ä»¤æ¨¡å¼è¯­éŸ³åˆæˆ - ç”Ÿæˆç²¤è¯­
        print("ğŸµ å¼€å§‹æµå¼ç²¤è¯­è¯­éŸ³åˆæˆ...")
        cosyvoice_instance = get_cosyvoice()

        # ğŸ”§ ç›´æ¥åŠ è½½æç¤ºéŸ³é¢‘
        print("ğŸµ åŠ è½½æç¤ºéŸ³é¢‘...")
        prompt_speech_16k = load_wav(f'{ROOT_DIR}/asset/zero_shot_prompt.wav', 16000)
        print("âœ… æç¤ºéŸ³é¢‘åŠ è½½å®Œæˆ")

        # ğŸ”§ å…³é”®ï¼šä½¿ç”¨æŒ‡ä»¤æ–‡æœ¬æ§åˆ¶è¯­éŸ³ç‰¹æ€§
        instruct_text = "è¯·ç”¨ç²¤è¯­è¯´è¯"  # æ˜ç¡®æŒ‡ä»¤ï¼šä½¿ç”¨ç²¤è¯­

        # ğŸ”§ é¢„å…ˆç”Ÿæˆå¥å­åˆ—è¡¨ï¼Œç„¶ååˆ›å»ºç”Ÿæˆå™¨å‡½æ•°
        print("ğŸ“ å¼€å§‹åˆ†å¥å¤„ç†...")
        sentences = list(_nlp_generator_optimized(answer_text))
        print(f"âœ… åˆ†å¥å®Œæˆï¼Œå…± {len(sentences)} ä¸ªå¥å­:")
        for i, sentence in enumerate(sentences, 1):
            print(f"   å¥å­{i}: ã€Œ{sentence}ã€")

        if not sentences:
            print("âŒ åˆ†å¥ç»“æœä¸ºç©º")
            return

        def create_text_generator():
            """åˆ›å»ºæ–‡æœ¬ç”Ÿæˆå™¨ - ç¡®ä¿åªç”Ÿæˆä¸€æ¬¡"""
            print("ğŸ¯ ç”Ÿæˆå™¨å¼€å§‹å·¥ä½œ...")
            for i, sentence in enumerate(sentences, 1):
                print(f"ğŸ¯ ç”Ÿæˆå™¨äº§å‡ºå¥å­ {i}: ã€Œ{sentence}ã€")
                yield sentence.strip()
            print("ğŸ¯ ç”Ÿæˆå™¨å·¥ä½œå®Œæˆ")

        # æµå¼éŸ³é¢‘ç”Ÿæˆå’Œæ’­æ”¾
        audio_chunk_count = 0
        min_chunk_duration = 0.5  # æœ€å°éŸ³é¢‘å—æ—¶é•¿ï¼ˆç§’ï¼‰ï¼Œé¿å…æ’­æ”¾è¿‡äºé¢‘ç¹
        accumulated_chunk = []

        print("ğŸµ å¼€å§‹æµå¼éŸ³é¢‘ç”Ÿæˆ...")
        print(f"ğŸ“‹ æŒ‡ä»¤æ–‡æœ¬: {instruct_text}")
        print("=" * 60)

        try:
            for i, output in enumerate(cosyvoice_instance.inference_instruct2(
                    create_text_generator(),
                    instruct_text,
                    prompt_speech_16k,
                    stream=True)):

                audio_chunk_count += 1
                chunk_start_time = time.time()

                print(f"ğŸµ æ­£åœ¨å¤„ç†ç¬¬ {audio_chunk_count} ä¸ªéŸ³é¢‘å—...")

                # å¤„ç†éŸ³é¢‘å—
                if 'tts_speech' not in output:
                    print(f"âš ï¸ éŸ³é¢‘å— {audio_chunk_count} ç¼ºå°‘ tts_speech")
                    continue

                audio_chunk = output['tts_speech'].cpu().numpy()

                # ç¡®ä¿æ•°æ®ç±»å‹ä¸ºfloat32
                if audio_chunk.dtype != np.float32:
                    audio_chunk = audio_chunk.astype(np.float32)

                # ç¡®ä¿éŸ³é¢‘ä¸º1Dæ•°ç»„
                if audio_chunk.ndim > 1:
                    audio_chunk = audio_chunk.flatten()

                # éŸ³é¢‘è´¨é‡ä¼˜åŒ–
                audio_chunk = np.clip(audio_chunk, -0.95, 0.95)

                if len(audio_chunk) > 0:
                    # å°†éŸ³é¢‘å—åŠ å…¥ç´¯ç§¯ç¼“å†²åŒº
                    accumulated_chunk.append(audio_chunk)

                    # æ£€æŸ¥ç´¯ç§¯çš„éŸ³é¢‘æ˜¯å¦è¾¾åˆ°æœ€å°æ’­æ”¾æ—¶é•¿
                    total_samples = sum(len(chunk) for chunk in accumulated_chunk)
                    total_duration = total_samples / 22050

                    chunk_duration = len(audio_chunk) / 22050
                    chunk_time = time.time() - chunk_start_time

                    print(f"âœ… éŸ³é¢‘å— {audio_chunk_count} å·²å¤„ç†")
                    print(f"   - å½“å‰å—é•¿åº¦: {len(audio_chunk)} æ ·æœ¬ ({chunk_duration:.2f}ç§’)")
                    print(f"   - ç´¯ç§¯æ—¶é•¿: {total_duration:.2f}ç§’")
                    print(f"   - å¤„ç†ç”¨æ—¶: {chunk_time:.2f}ç§’")

                    # ğŸ”§ å…³é”®ï¼šå½“ç´¯ç§¯éŸ³é¢‘è¾¾åˆ°æœ€å°æ’­æ”¾æ—¶é•¿æ—¶ï¼Œè¿”å›ç»™ç•Œé¢æ’­æ”¾
                    if total_duration >= min_chunk_duration or audio_chunk_count % 3 == 0:
                        # åˆå¹¶ç´¯ç§¯çš„éŸ³é¢‘å—
                        if len(accumulated_chunk) == 1:
                            playback_audio = accumulated_chunk[0]
                        else:
                            playback_audio = np.concatenate(accumulated_chunk)

                        # åº”ç”¨åŸºæœ¬çš„éŸ³é¢‘ä¼˜åŒ–
                        playback_audio = np.clip(playback_audio, -0.95, 0.95)
                        playback_audio = trim_silence(playback_audio, threshold=0.005)

                        # è½»é‡çº§éŸ³é¢‘å¢å¼ºï¼ˆé¿å…è¿‡åº¦å¤„ç†ï¼‰
                        max_val = np.max(np.abs(playback_audio))
                        if max_val > 0:
                            playback_audio = playback_audio / max_val * 0.7

                        playback_duration = len(playback_audio) / 22050

                        print(f"ğŸµ è¿”å›éŸ³é¢‘å—ç”¨äºæ’­æ”¾:")
                        print(f"   - æ’­æ”¾æ—¶é•¿: {playback_duration:.2f}ç§’")
                        print(f"   - åŒ…å« {len(accumulated_chunk)} ä¸ªåŸå§‹éŸ³é¢‘å—")

                        # è¿”å›éŸ³é¢‘å—ä¾›ç•Œé¢æ’­æ”¾
                        yield (22050, playback_audio)

                        # æ¸…ç©ºç´¯ç§¯ç¼“å†²åŒºï¼Œä¸ºä¸‹ä¸€è½®åšå‡†å¤‡
                        accumulated_chunk = []

                        # æ·»åŠ çŸ­æš‚åœé¡¿ï¼Œè®©ç•Œé¢æœ‰æ—¶é—´å¤„ç†
                        time.sleep(0.1)

                else:
                    print(f"âš ï¸ éŸ³é¢‘å— {audio_chunk_count} ä¸ºç©ºï¼Œè·³è¿‡")

                # å†…å­˜ç®¡ç†
                if audio_chunk_count % 5 == 0:
                    optimize_memory()

            # å¤„ç†å‰©ä½™çš„ç´¯ç§¯éŸ³é¢‘ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            if accumulated_chunk:
                print(f"ğŸ”— å¤„ç†å‰©ä½™çš„ {len(accumulated_chunk)} ä¸ªéŸ³é¢‘å—...")

                if len(accumulated_chunk) == 1:
                    final_audio = accumulated_chunk[0]
                else:
                    final_audio = np.concatenate(accumulated_chunk)

                # æœ€ç»ˆéŸ³é¢‘ä¼˜åŒ–
                final_audio = np.clip(final_audio, -0.95, 0.95)
                final_audio = trim_silence(final_audio, threshold=0.005)

                # è½»é‡çº§å¢å¼º
                max_val = np.max(np.abs(final_audio))
                if max_val > 0:
                    final_audio = final_audio / max_val * 0.7

                final_duration = len(final_audio) / 22050
                print(f"ğŸµ è¿”å›æœ€ç»ˆéŸ³é¢‘å—: {final_duration:.2f}ç§’")

                yield (22050, final_audio)

            total_time = time.time() - start_time
            print("=" * 60)
            print(f"âœ… æµå¼éŸ³é¢‘ç”Ÿæˆå®Œæˆ:")
            print(f"   - æ€»éŸ³é¢‘å—æ•°: {audio_chunk_count}")
            print(f"   - æ€»å¤„ç†æ—¶é—´: {total_time:.2f}ç§’")
            print(f"   - ä½¿ç”¨æŒ‡ä»¤: {instruct_text}")
            print("=" * 60)

        except Exception as e:
            print(f"âŒ æµå¼éŸ³é¢‘ç”Ÿæˆå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return

    except Exception as e:
        print(f"âŒ æµå¼éŸ³é¢‘å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return

# ç®€åŒ–çš„CSSæ ·å¼ - é»‘è‰²èƒŒæ™¯ï¼Œç™½è‰²å­—ä½“ï¼ŒåŒéŸ³é¢‘æ§ä»¶è®¾è®¡ - 13å¯¸å±å¹•é€‚é…
def create_custom_css():
    return """
    /* å…¨å±€æ ·å¼ - é»‘è‰²èƒŒæ™¯ä¸»é¢˜ */
    .gradio-container {
        background: #000000 !important;
        color: #ffffff !important;
        min-height: 100vh !important;
        padding: 10px !important;
        font-family: 'Arial', sans-serif !important;
        font-size: 14px !important;
    }
    
    /* ä¸»å®¹å™¨æ ·å¼ - 13å¯¸å±å¹•ä¼˜åŒ– */
    .main-container {
        max-width: 900px !important;
        margin: 0 auto !important;
        background: linear-gradient(135deg, #000000, #1a1a1a) !important;
        border-radius: 15px !important;
        padding: 20px !important;
        box-shadow: 0 5px 20px rgba(0,0,0,0.5) !important;
        max-height: 95vh !important;
        overflow-y: auto !important;
    }
    
    /* æ ‡é¢˜æ ·å¼ - ç¼©å°å°ºå¯¸ */
    .main-title {
        font-size: 2rem !important;
        font-weight: bold !important;
        color: #ffffff !important;
        text-align: center !important;
        margin-bottom: 20px !important;
        text-shadow: 0 0 15px rgba(255,255,255,0.5) !important;
    }
    
    /* çŠ¶æ€æ˜¾ç¤ºæ ·å¼ - ç¼©å°å°ºå¯¸ */
    .status-display {
        background: rgba(255,255,255,0.1) !important;
        border: 2px solid #ffffff !important;
        border-radius: 10px !important;
        padding: 12px 15px !important;
        margin: 10px 0 !important;
        font-size: 1rem !important;
        color: #ffffff !important;
        text-align: center !important;
        backdrop-filter: blur(10px) !important;
    }
    
    /* éŸ³é¢‘åŒºåŸŸå®¹å™¨ - ç¼©å°é—´è· */
    .audio-section {
        background: rgba(255,255,255,0.05) !important;
        border: 1px solid rgba(255,255,255,0.2) !important;
        border-radius: 10px !important;
        padding: 15px !important;
        margin: 10px 0 !important;
        backdrop-filter: blur(5px) !important;
    }
    
    /* éŸ³é¢‘åŒºåŸŸæ ‡é¢˜ - ç¼©å°å­—ä½“ */
    .audio-title {
        font-size: 1.2rem !important;
        font-weight: bold !important;
        color: #4CAF50 !important;
        text-align: center !important;
        margin-bottom: 10px !important;
        display: block !important;
    }
    
    /* å½•éŸ³æ§åˆ¶æŒ‰é’® - ç¼©å°é«˜åº¦ */
    .record-button {
        background: linear-gradient(135deg, #4CAF50, #45a049) !important;
        color: white !important;
        border: none !important;
        min-height: 50px !important;
        width: 100% !important;
        font-size: 1.1rem !important;
        font-weight: bold !important;
        border-radius: 10px !important;
        cursor: pointer !important;
        transition: all 0.3s ease !important;
        box-shadow: 0 4px 8px rgba(0,0,0,0.3) !important;
        margin: 8px 0 !important;
    }
    
    .record-button:hover {
        transform: translateY(-1px) !important;
        box-shadow: 0 6px 12px rgba(0,0,0,0.4) !important;
    }
    
    /* åœæ­¢å½•éŸ³æŒ‰é’® */
    .stop-button {
        background: linear-gradient(135deg, #F44336, #d32f2f) !important;
        animation: pulse-red 1.5s infinite !important;
    }
    
    @keyframes pulse-red {
        0% { box-shadow: 0 4px 8px rgba(244,67,54,0.3); }
        50% { box-shadow: 0 4px 8px rgba(244,67,54,0.7), 0 0 15px rgba(244,67,54,0.5); }
        100% { box-shadow: 0 4px 8px rgba(244,67,54,0.3); }
    }
    
    /* å¤„ç†ä¸­æŒ‰é’® */
    .processing-button {
        background: linear-gradient(135deg, #FF9800, #F57C00) !important;
        cursor: not-allowed !important;
        animation: pulse-orange 1s infinite !important;
    }
    
    @keyframes pulse-orange {
        0% { opacity: 0.8; }
        50% { opacity: 1; }
        100% { opacity: 0.8; }
    }
    
    /* éŸ³é¢‘æ’­æ”¾å™¨æ ·å¼ - ç¼©å°å°ºå¯¸ */
    .audio-player {
        background: rgba(255,255,255,0.1) !important;
        border: 1px solid rgba(255,255,255,0.3) !important;
        border-radius: 8px !important;
        padding: 10px !important;
        margin: 8px 0 !important;
    }
    
    /* éŸ³é¢‘æ’­æ”¾å™¨å†…éƒ¨æ§ä»¶è°ƒæ•´ */
    .audio-player audio {
        width: 100% !important;
        height: 40px !important;
    }
    
    /* ğŸ”§ æ–°å¢ï¼šé‡ç½®æŒ‰é’®æ ·å¼ */
    .reset-button {
        background: linear-gradient(135deg, #FF9800, #F57C00) !important;
        color: white !important;
        border: none !important;
        min-height: 35px !important;
        width: 100% !important;
        font-size: 0.9rem !important;
        font-weight: bold !important;
        border-radius: 8px !important;
        cursor: pointer !important;
        transition: all 0.3s ease !important;
        box-shadow: 0 2px 6px rgba(255,152,0,0.3) !important;
        margin-top: 8px !important;
    }
    
    .reset-button:hover {
        background: linear-gradient(135deg, #F57C00, #E65100) !important;
        transform: translateY(-1px) !important;
        box-shadow: 0 4px 8px rgba(255,152,0,0.4) !important;
    }
    
    .reset-button:active {
        transform: translateY(0px) !important;
        box-shadow: 0 2px 4px rgba(255,152,0,0.3) !important;
    }
    
    /* Jarvisè¾“å‡ºéŸ³é¢‘æ ‡é¢˜ */
    .jarvis-title {
        color: #2196F3 !important;
    }
    
    /* ç”¨æˆ·å½•éŸ³éŸ³é¢‘æ ‡é¢˜ */
    .user-title {
        color: #4CAF50 !important;
    }
    
    /* æµå¼æ’­æ”¾æç¤º - ç¼©å°å°ºå¯¸ */
    .streaming-indicator {
        background: linear-gradient(135deg, #2196F3, #1976D2) !important;
        color: white !important;
        padding: 8px 15px !important;
        border-radius: 15px !important;
        font-size: 0.9rem !important;
        text-align: center !important;
        margin: 8px 0 !important;
        animation: pulse-blue 2s infinite !important;
    }
    
    @keyframes pulse-blue {
        0% { transform: scale(1); opacity: 0.8; }
        50% { transform: scale(1.01); opacity: 1; }
        100% { transform: scale(1); opacity: 0.8; }
    }
    
    /* Gradioé»˜è®¤æ ·å¼è°ƒæ•´ */
    .gradio-container .gradio-row {
        margin: 5px 0 !important;
    }
    
    .gradio-container .gradio-column {
        padding: 5px !important;
    }
    
    /* éŸ³é¢‘ç»„ä»¶æ ‡ç­¾æ ·å¼ */
    .gradio-container label {
        font-size: 0.9rem !important;
        margin-bottom: 5px !important;
    }
    
    /* æ»šåŠ¨æ¡æ ·å¼ */
    ::-webkit-scrollbar {
        width: 8px !important;
    }
    
    ::-webkit-scrollbar-track {
        background: rgba(255,255,255,0.1) !important;
    }
    
    ::-webkit-scrollbar-thumb {
        background: rgba(255,255,255,0.3) !important;
        border-radius: 4px !important;
    }
    
    /* 13å¯¸å±å¹•ç‰¹æ®Šä¼˜åŒ– */
    @media (max-height: 800px) {
        .main-container {
            padding: 15px !important;
            max-height: 98vh !important;
        }
        
        .main-title {
            font-size: 1.8rem !important;
            margin-bottom: 15px !important;
        }
        
        .audio-section {
            padding: 12px !important;
            margin: 8px 0 !important;
        }
        
        .status-display {
            padding: 10px 12px !important;
            font-size: 0.95rem !important;
        }
    }
    
    /* å°å±å¹•å“åº”å¼è®¾è®¡ */
    @media (max-width: 768px) {
        .main-container {
            max-width: 95% !important;
            padding: 12px !important;
        }
        
        .main-title {
            font-size: 1.5rem !important;
        }
        
        .record-button {
            min-height: 45px !important;
            font-size: 1rem !important;
        }
        
        .audio-title {
            font-size: 1.1rem !important;
        }
    }
    """

def create_elderly_friendly_interface():
    """åˆ›å»ºä½¿ç”¨GradioåŸç”ŸéŸ³é¢‘æ§ä»¶çš„ç®€åŒ–ç•Œé¢"""

    custom_css = create_custom_css()

    with gr.Blocks(
        theme=gr.themes.Base(),
        css=custom_css,
        title="Jarvis ç²¤è¯­è¯­éŸ³åŠ©æ‰‹"
    ) as interface:

        # ä¸»å®¹å™¨
        with gr.Column(elem_classes=["main-container"]):

            # æ ‡é¢˜
            gr.HTML("""
            <div class="main-title">
                ğŸ¤– Jarvis ç²¤è¯­è¯­éŸ³åŠ©æ‰‹
            </div>
            """)

            # å…¨å±€çŠ¶æ€æ˜¾ç¤º
            status_display = gr.HTML(
                value='<div class="status-display">ğŸ‘‹ å‡†å¤‡å°±ç»ªï¼Œè¯·ç‚¹å‡»ä¸‹æ–¹å½•éŸ³æŒ‰é’®å¼€å§‹å¯¹è¯</div>'
            )

            # ç”¨æˆ·å½•éŸ³åŒºåŸŸ
            with gr.Column(elem_classes=["audio-section"]):
                gr.HTML('<div class="audio-title user-title">ğŸ¤ æ‚¨çš„è¯­éŸ³è¾“å…¥</div>')

                # ä½¿ç”¨GradioåŸç”ŸéŸ³é¢‘å½•åˆ¶æ§ä»¶
                user_audio = gr.Audio(
                    sources=["microphone"],
                    type="numpy",
                    label="ç‚¹å‡»ğŸ¤å½•éŸ³æŒ‰é’®å¼€å§‹è¯´è¯",
                    show_label=True,
                    elem_classes=["audio-player"],
                    interactive=True
                )

                # ğŸ”§ æ–°å¢ï¼šé‡ç½®æŒ‰é’®
                with gr.Row():
                    reset_button = gr.Button(
                        value="ğŸ”„ é‡ç½®å½•éŸ³",
                        variant="secondary",
                        size="sm",
                        elem_classes=["reset-button"]
                    )

            # Jarviså›å¤åŒºåŸŸ
            with gr.Column(elem_classes=["audio-section"]):
                gr.HTML('<div class="audio-title jarvis-title">ğŸ”Š Jarvis è¯­éŸ³å›å¤</div>')

                # æµå¼æ’­æ”¾çŠ¶æ€æŒ‡ç¤º
                streaming_indicator = gr.HTML(
                    value="",
                    visible=False
                )

                # Jarvisè¾“å‡ºéŸ³é¢‘ç»„ä»¶
                jarvis_audio = gr.Audio(
                    label="Jarviså›å¤éŸ³é¢‘",
                    show_label=True,
                    elem_classes=["audio-player"],
                    autoplay=True,
                    interactive=False
                )

        # éŸ³é¢‘å¤„ç†å‡½æ•° - å½“ç”¨æˆ·å®Œæˆå½•éŸ³åè‡ªåŠ¨è§¦å‘
        def process_audio_input(audio_data):
            print(f"ğŸµ æ”¶åˆ°éŸ³é¢‘æ•°æ®: {type(audio_data) if audio_data else 'None'}")

            if audio_data is None:
                print("âŒ éŸ³é¢‘æ•°æ®ä¸ºç©º")
                return (
                    gr.update(value='<div class="status-display">âŒ å½•éŸ³å¤±è´¥ï¼Œè¯·é‡è¯•</div>'),
                    "",
                    None
                )

            try:
                # æ›´æ–°çŠ¶æ€ä¸ºå¤„ç†ä¸­
                yield (
                    gr.update(value='<div class="status-display">ğŸ§  æ­£åœ¨ç†è§£æ‚¨çš„è¯...</div>'),
                    "",
                    None
                )

                # æ­¥éª¤1: è¯­éŸ³è¯†åˆ«
                print("ğŸ¤ å¼€å§‹è¯­éŸ³è¯†åˆ«...")
                prompt_text = listener(audio_data)
                print(f"âœ… è¯†åˆ«ç»“æœ: {prompt_text}")

                yield (
                    gr.update(value=f'<div class="status-display">ğŸ“ è¯†åˆ«åˆ°ï¼š{prompt_text}</div>'),
                    "",
                    None
                )

                # æ­¥éª¤2: LLMç”Ÿæˆå›å¤
                print("ğŸ§  å¼€å§‹LLMç”Ÿæˆå›å¤...")
                answer_text = Influence.llm(prompt_text)
                print(f"âœ… LLMå›å¤: {answer_text}")

                yield (
                    gr.update(value='<div class="status-display">ğŸµ æ­£åœ¨ç”Ÿæˆè¯­éŸ³å›å¤...</div>'),
                    '<div class="streaming-indicator">ğŸ”„ æ­£åœ¨æµå¼ç”Ÿæˆè¯­éŸ³ï¼Œè¯·ç¨å€™...</div>',
                    None
                )

                # æ­¥éª¤3: çœŸæ­£çš„æµå¼è¯­éŸ³åˆæˆå¹¶æ’­æ”¾
                print("ğŸµ å¼€å§‹æµå¼è¯­éŸ³åˆæˆ...")

                # ğŸ”§ æ¢å¤æµå¼æ’­æ”¾ï¼šæ¯ä¸ªéŸ³é¢‘å—éƒ½å®æ—¶æ¨é€ç»™Gradioæ’­æ”¾
                streaming_audio_generator = brain_streaming_with_realtime_playback(audio_data)

                chunk_count = 0

                # çœŸæ­£çš„æµå¼æ’­æ”¾ï¼šæ¯ä¸ªéŸ³é¢‘å—éƒ½ç«‹å³æ’­æ”¾
                for audio_chunk_result in streaming_audio_generator:
                    if audio_chunk_result is None:
                        continue

                    chunk_count += 1
                    sample_rate, audio_chunk = audio_chunk_result

                    # å®æ—¶æµå¼æ’­æ”¾æ¯ä¸ªéŸ³é¢‘å—
                    yield (
                        gr.update(value=f'<div class="status-display">ğŸµ æ­£åœ¨æ’­æ”¾ç¬¬ {chunk_count} æ®µè¯­éŸ³...</div>'),
                        '<div class="streaming-indicator">ğŸµ æµå¼æ’­æ”¾ä¸­...</div>',
                        (sample_rate, audio_chunk)  # å®æ—¶æ’­æ”¾å½“å‰éŸ³é¢‘å—
                    )

                    print(f"âœ… æµå¼æ’­æ”¾ï¼šç¬¬ {chunk_count} æ®µéŸ³é¢‘å·²æ¨é€åˆ°ç•Œé¢æ’­æ”¾")

                # æ’­æ”¾å®ŒæˆçŠ¶æ€
                yield (
                    gr.update(value='<div class="status-display">âœ… å¯¹è¯å®Œæˆï¼å¯ä»¥ç»§ç»­å½•éŸ³è¿›è¡Œä¸‹ä¸€è½®å¯¹è¯</div>'),
                    "",  # éšè—æµå¼æŒ‡ç¤ºå™¨
                    None  # æ¸…ç©ºéŸ³é¢‘ç»„ä»¶
                )

            except Exception as e:
                print(f"âŒ å¤„ç†å¼‚å¸¸: {e}")
                import traceback
                traceback.print_exc()
                yield (
                    gr.update(value=f'<div class="status-display">âŒ å¤„ç†å¤±è´¥: {str(e)}</div>'),
                    "",
                    None
                )

        # ğŸ”§ æ–°å¢ï¼šé‡ç½®å½•éŸ³åŠŸèƒ½
        def reset_recording():
            """é‡ç½®å½•éŸ³å’ŒçŠ¶æ€"""
            print("ğŸ”„ ç”¨æˆ·ç‚¹å‡»é‡ç½®å½•éŸ³")
            return (
                None,  # æ¸…ç©ºç”¨æˆ·å½•éŸ³
                gr.update(value='<div class="status-display">ğŸ”„ å½•éŸ³å·²é‡ç½®ï¼Œè¯·é‡æ–°å½•éŸ³</div>'),
                "",    # æ¸…ç©ºæµå¼æŒ‡ç¤ºå™¨
                None   # æ¸…ç©ºJarviséŸ³é¢‘
            )

        # ç»‘å®šé‡ç½®æŒ‰é’®äº‹ä»¶
        reset_button.click(
            fn=reset_recording,
            inputs=[],
            outputs=[user_audio, status_display, streaming_indicator, jarvis_audio]
        )

        # ç»‘å®šéŸ³é¢‘å½•åˆ¶å®Œæˆäº‹ä»¶ - å½“ç”¨æˆ·å½•éŸ³å®Œæˆåè‡ªåŠ¨å¤„ç†
        user_audio.change(
            fn=process_audio_input,
            inputs=[user_audio],
            outputs=[status_display, streaming_indicator, jarvis_audio],
            show_progress="hidden"
        )

    return interface

def ui_launch():
    """å¯åŠ¨ä¼˜åŒ–åçš„ç”¨æˆ·ç•Œé¢"""
    print("ğŸš€ å¯åŠ¨ Jarvis ç²¤è¯­è¯­éŸ³åŠ©æ‰‹...")
    print("âœ¨ ç•Œé¢ä¼˜åŒ–ç‰¹æ€§:")
    print("   - é€‚åˆè€å¹´ç”¨æˆ·çš„å¤§æŒ‰é’®è®¾è®¡")
    print("   - æ¸…æ™°çš„çŠ¶æ€æŒ‡ç¤ºå™¨")
    print("   - ä¸€é”®å½•éŸ³å¼€å§‹/åœæ­¢åŠŸèƒ½")
    print("   - ç®€æ´çš„è‰²å½©æ­é…")
    print("   - å®æ—¶çŠ¶æ€åé¦ˆ")
    print("   - å“åº”å¼å¸ƒå±€é€‚é…")
    print("   - æ— éšœç¢ä¼˜åŒ–æ”¯æŒ")

    interface = create_elderly_friendly_interface()

    # å¯åŠ¨ç•Œé¢
    interface.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        show_error=True,
        inbrowser=True,
        quiet=False
    )

if __name__ == "__main__":
    """ä¸»å‡½æ•°å…¥å£"""
    try:
        print("ğŸ¯ Jarvis ç²¤è¯­è¯­éŸ³äº¤äº’ç³»ç»Ÿ v2.0")
        print("=" * 50)
        print("ğŸš€ ç³»ç»Ÿå¯åŠ¨ä¸­...")
        print("ğŸ“ æ­£åœ¨åŠ è½½ç»„ä»¶...")

        # é¢„åˆå§‹åŒ–å…³é”®ç»„ä»¶ä»¥åŠ å¿«å“åº”é€Ÿåº¦
        print("âš¡ é¢„åˆå§‹åŒ–ç»„ä»¶ä¸­...")

        # å¯åŠ¨ç”¨æˆ·ç•Œé¢
        ui_launch()

    except KeyboardInterrupt:
        print("\nğŸ‘‹ ç”¨æˆ·ç»ˆæ­¢ç¨‹åº")
    except Exception as e:
        print(f"âŒ å¯åŠ¨å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    finally:
        # æ¸…ç†èµ„æº
        try:
            executor.shutdown(wait=False)
            if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                torch.backends.mps.empty_cache()
        except:
            pass
        print("ğŸ”š ç¨‹åºå·²é€€å‡º")
