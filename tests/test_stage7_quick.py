#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é˜¶æ®µ7ï¼šç³»ç»Ÿé›†æˆæµ‹è¯• - å¿«é€ŸéªŒè¯ç‰ˆæœ¬
"""

import os
import sys
import time
import json
from datetime import datetime

ROOT_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(ROOT_DIR)

def main():
    print("ğŸ§ª Jarvisç³»ç»Ÿé›†æˆæµ‹è¯• - é˜¶æ®µ7")
    print("=" * 50)

    test_results = {
        'timestamp': datetime.now().isoformat(),
        'tests_completed': [],
        'tests_passed': [],
        'tests_failed': [],
        'performance_metrics': {}
    }

    # æµ‹è¯•1ï¼šLLMç²¤è¯­åŠŸèƒ½
    print("\nğŸ’¬ æµ‹è¯•1ï¼šLLMç²¤è¯­åŠŸèƒ½")
    try:
        from influence.influence import Influence
        influence = Influence()

        test_cases = [
            "ä½ å¥½",
            "ä»Šæ—¥å¤©æ°”ç‚¹æ ·ï¼Ÿ",
            "å¸®æˆ‘è®²ä¸ªæ•…äº‹"
        ]

        for i, text in enumerate(test_cases):
            start_time = time.time()
            response = influence.llm(text)
            response_time = time.time() - start_time

            print(f"  è¾“å…¥: {text}")
            print(f"  å›å¤: {response}")
            print(f"  è€—æ—¶: {response_time:.2f}s")

            # æ£€æŸ¥ç²¤è¯­ç‰¹å¾
            cantonese_indicators = ['ä¿‚', 'å˜…', 'å’©', 'å””', 'ä½¢', 'å–º', 'å’']
            cantonese_score = sum(1 for indicator in cantonese_indicators if indicator in response)
            print(f"  ç²¤è¯­å¾—åˆ†: {cantonese_score}/{len(cantonese_indicators)}")
            print()

        test_results['tests_completed'].append('llm_cantonese')
        test_results['tests_passed'].append('llm_cantonese')
        test_results['performance_metrics']['avg_llm_time'] = response_time
        print("âœ… LLMç²¤è¯­åŠŸèƒ½æµ‹è¯•é€šè¿‡")

    except Exception as e:
        print(f"âŒ LLMæµ‹è¯•å¤±è´¥: {e}")
        test_results['tests_completed'].append('llm_cantonese')
        test_results['tests_failed'].append('llm_cantonese')

    # æµ‹è¯•2ï¼šè¯­éŸ³è¯†åˆ«åŠŸèƒ½
    print("\nğŸ¤ æµ‹è¯•2ï¼šè¯­éŸ³è¯†åˆ«åŠŸèƒ½")
    try:
        recordings_dir = os.path.join(ROOT_DIR, 'recordings')
        if os.path.exists(recordings_dir):
            audio_files = [f for f in os.listdir(recordings_dir) if f.endswith('.wav')][:2]

            if audio_files:
                recognition_times = []
                for audio_file in audio_files:
                    audio_path = os.path.join(recordings_dir, audio_file)
                    start_time = time.time()
                    result = influence.voice_to_text(audio_path)
                    recognition_time = time.time() - start_time
                    recognition_times.append(recognition_time)

                    print(f"  æ–‡ä»¶: {audio_file}")
                    print(f"  è¯†åˆ«: {result}")
                    print(f"  è€—æ—¶: {recognition_time:.2f}s")

                avg_recognition_time = sum(recognition_times) / len(recognition_times)
                test_results['performance_metrics']['avg_recognition_time'] = avg_recognition_time

                test_results['tests_completed'].append('speech_recognition')
                test_results['tests_passed'].append('speech_recognition')
                print("âœ… è¯­éŸ³è¯†åˆ«åŠŸèƒ½æµ‹è¯•é€šè¿‡")
            else:
                print("âš ï¸  æ— éŸ³é¢‘æ–‡ä»¶ï¼Œè·³è¿‡æµ‹è¯•")
        else:
            print("âš ï¸  recordingsç›®å½•ä¸å­˜åœ¨ï¼Œè·³è¿‡æµ‹è¯•")

    except Exception as e:
        print(f"âŒ è¯­éŸ³è¯†åˆ«æµ‹è¯•å¤±è´¥: {e}")
        test_results['tests_completed'].append('speech_recognition')
        test_results['tests_failed'].append('speech_recognition')

    # æµ‹è¯•3ï¼šè¯­éŸ³åˆæˆåŠŸèƒ½ï¼ˆç®€åŒ–ç‰ˆï¼‰
    print("\nğŸ”Š æµ‹è¯•3ï¼šè¯­éŸ³åˆæˆåŠŸèƒ½")
    try:
        from jarvis import initialize_cosyvoice_optimized
        from cosyvoice.utils.file_utils import load_wav

        print("  åˆå§‹åŒ–CosyVoice2...")
        cosyvoice = initialize_cosyvoice_optimized()

        # åŠ è½½æç¤ºè¯­éŸ³
        prompt_speech_16k = load_wav(f'{ROOT_DIR}/asset/zero_shot_prompt.wav', 16000)

        # æµ‹è¯•ç®€çŸ­æ–‡æœ¬åˆæˆ
        test_text = "æµ‹è¯•"
        print(f"  åˆæˆæ–‡æœ¬: {test_text}")

        start_time = time.time()
        chunk_count = 0

        for chunk in cosyvoice.inference_instruct2(
            tts_text=test_text,
            instruct_text="å¥³æ€§ï¼Œæ¸©æŸ”ï¼Œç²¤è¯­",
            prompt_speech_16k=prompt_speech_16k,
            stream=True
        ):
            if chunk is not None:
                chunk_count += 1
            if chunk_count >= 2:  # é™åˆ¶æµ‹è¯•é•¿åº¦
                break

        synthesis_time = time.time() - start_time

        print(f"  ç”ŸæˆéŸ³é¢‘å—: {chunk_count}")
        print(f"  åˆæˆè€—æ—¶: {synthesis_time:.2f}s")

        test_results['performance_metrics']['synthesis_time'] = synthesis_time
        test_results['tests_completed'].append('speech_synthesis')

        if chunk_count > 0:
            test_results['tests_passed'].append('speech_synthesis')
            print("âœ… è¯­éŸ³åˆæˆåŠŸèƒ½æµ‹è¯•é€šè¿‡")
        else:
            test_results['tests_failed'].append('speech_synthesis')
            print("âŒ è¯­éŸ³åˆæˆæœªäº§ç”ŸéŸ³é¢‘å—")

    except Exception as e:
        print(f"âŒ è¯­éŸ³åˆæˆæµ‹è¯•å¤±è´¥: {e}")
        test_results['tests_completed'].append('speech_synthesis')
        test_results['tests_failed'].append('speech_synthesis')

    # æµ‹è¯•4ï¼šç«¯åˆ°ç«¯æ€§èƒ½æµ‹è¯•
    print("\nğŸ”„ æµ‹è¯•4ï¼šç«¯åˆ°ç«¯æ€§èƒ½æµ‹è¯•")
    try:
        total_start = time.time()

        # LLMå¤„ç†
        llm_start = time.time()
        response = influence.llm("ä½ å¥½")
        llm_time = time.time() - llm_start

        # ç®€çŸ­è¯­éŸ³åˆæˆ
        synthesis_start = time.time()
        chunk_count = 0
        for chunk in cosyvoice.inference_instruct2(
            tts_text=response[:20],  # åªåˆæˆå‰20å­—ç¬¦
            instruct_text="å¥³æ€§ï¼Œæ¸©æŸ”ï¼Œç²¤è¯­",
            prompt_speech_16k=prompt_speech_16k,
            stream=True
        ):
            if chunk is not None:
                chunk_count += 1
            if chunk_count >= 2:
                break
        synthesis_time = time.time() - synthesis_start

        total_time = time.time() - total_start

        print(f"  LLMæ—¶é—´: {llm_time:.2f}s")
        print(f"  åˆæˆæ—¶é—´: {synthesis_time:.2f}s")
        print(f"  æ€»æ—¶é—´: {total_time:.2f}s")

        test_results['performance_metrics'].update({
            'end_to_end_llm_time': llm_time,
            'end_to_end_synthesis_time': synthesis_time,
            'end_to_end_total_time': total_time
        })

        test_results['tests_completed'].append('end_to_end')

        if total_time < 10:  # æ€§èƒ½ç›®æ ‡ï¼š10ç§’å†…
            test_results['tests_passed'].append('end_to_end')
            print("âœ… ç«¯åˆ°ç«¯æ€§èƒ½æµ‹è¯•é€šè¿‡")
        else:
            test_results['tests_failed'].append('end_to_end')
            print(f"âŒ ç«¯åˆ°ç«¯æ€§èƒ½è¶…æ—¶: {total_time:.2f}s > 10s")

    except Exception as e:
        print(f"âŒ ç«¯åˆ°ç«¯æµ‹è¯•å¤±è´¥: {e}")
        test_results['tests_completed'].append('end_to_end')
        test_results['tests_failed'].append('end_to_end')

    # æ±‡æ€»ç»“æœ
    print("\n" + "=" * 50)
    print("ğŸ“Š é˜¶æ®µ7ç³»ç»Ÿé›†æˆæµ‹è¯•ç»“æœ")
    print("=" * 50)

    total_tests = len(test_results['tests_completed'])
    passed_tests = len(test_results['tests_passed'])
    failed_tests = len(test_results['tests_failed'])
    success_rate = (passed_tests / total_tests * 100) if total_tests > 0 else 0

    print(f"æ€»æµ‹è¯•æ•°: {total_tests}")
    print(f"é€šè¿‡æµ‹è¯•: {passed_tests}")
    print(f"å¤±è´¥æµ‹è¯•: {failed_tests}")
    print(f"é€šè¿‡ç‡: {success_rate:.1f}%")

    print("\nğŸ“ˆ æ€§èƒ½æŒ‡æ ‡:")
    for key, value in test_results['performance_metrics'].items():
        print(f"  {key}: {value:.2f}s")

    # éªŒæ”¶æ ‡å‡†æ£€æŸ¥
    print("\nâœ… éªŒæ”¶æ ‡å‡†æ£€æŸ¥:")
    checks = []

    # æ£€æŸ¥1ï¼šLLMç”Ÿæˆç²¤è¯­
    if 'llm_cantonese' in test_results['tests_passed']:
        checks.append("âœ… LLMç”Ÿæˆçº¯ç²¤è¯­å›å¤")
    else:
        checks.append("âŒ LLMç”Ÿæˆçº¯ç²¤è¯­å›å¤")

    # æ£€æŸ¥2ï¼šç«¯åˆ°ç«¯å“åº”æ—¶é—´<10ç§’
    end_to_end_time = test_results['performance_metrics'].get('end_to_end_total_time', float('inf'))
    if end_to_end_time < 10:
        checks.append("âœ… ç«¯åˆ°ç«¯å“åº”æ—¶é—´<10ç§’")
    else:
        checks.append(f"âŒ ç«¯åˆ°ç«¯å“åº”æ—¶é—´>10ç§’ ({end_to_end_time:.2f}s)")

    # æ£€æŸ¥3ï¼šæ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½æ­£å¸¸
    core_functions = ['llm_cantonese', 'speech_synthesis']
    core_working = all(func in test_results['tests_passed'] for func in core_functions if func in test_results['tests_completed'])
    if core_working:
        checks.append("âœ… æ ¸å¿ƒåŠŸèƒ½æ­£å¸¸å·¥ä½œ")
    else:
        checks.append("âŒ éƒ¨åˆ†æ ¸å¿ƒåŠŸèƒ½å¼‚å¸¸")

    for check in checks:
        print(f"  {check}")

    # ä¿å­˜æµ‹è¯•æŠ¥å‘Š
    report_path = os.path.join(ROOT_DIR, 'tests', 'stage7_integration_report.json')
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(test_results, f, indent=2, ensure_ascii=False)

    print(f"\nğŸ“„ è¯¦ç»†æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜: {report_path}")

    # æœ€ç»ˆç»“è®º
    if success_rate >= 75 and end_to_end_time < 10:
        print("\nğŸ‰ é˜¶æ®µ7ç³»ç»Ÿé›†æˆæµ‹è¯•åŸºæœ¬é€šè¿‡ï¼")
        print("ç³»ç»Ÿå·²å…·å¤‡åŸºç¡€çš„ç²¤è¯­è¯­éŸ³äº¤äº’èƒ½åŠ›")
        return True
    else:
        print("\nâš ï¸  é˜¶æ®µ7ç³»ç»Ÿé›†æˆæµ‹è¯•éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")
        return False

if __name__ == '__main__':
    success = main()
    exit(0 if success else 1)
